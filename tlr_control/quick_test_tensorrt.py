#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TensorRTç­–ç•¥æ¨¡å‹å¿«é€Ÿæµ‹è¯•è„šæœ¬
éªŒè¯TensorRTæ¨¡å‹åŠ è½½ã€è½¬æ¢å’Œæ¨ç†åŠŸèƒ½
"""

import os
import sys
import numpy as np

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

try:
    import tensorrt as trt
    import pycuda.driver as cuda
    from model import PolicyModel
    TENSORRT_AVAILABLE = True
except ImportError as e:
    TENSORRT_AVAILABLE = False
    IMPORT_ERROR = str(e)

def check_environment():
    """æ£€æŸ¥TensorRTç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥TensorRTç¯å¢ƒ...")
    
    if not TENSORRT_AVAILABLE:
        print(f"   âŒ TensorRTä¸å¯ç”¨: {IMPORT_ERROR}")
        print("   ğŸ’¡ è¯·å®‰è£…TensorRT:")
        print("      1. ä¸‹è½½TensorRT (é€‚ç”¨äºJetson)")
        print("      2. pip install tensorrt")
        print("      3. pip install pycuda")
        return False
    
    print(f"   âœ… TensorRTç‰ˆæœ¬: {trt.__version__}")
    
    # æ£€æŸ¥CUDA
    try:
        cuda.init()
        device_count = cuda.Device.count()
        if device_count == 0:
            print("   âŒ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
            return False
        
        device = cuda.Device(0)
        print(f"   âœ… CUDAè®¾å¤‡: {device.name()}")
        print(f"   âœ… è®¾å¤‡æ•°é‡: {device_count}")
        
        # è·å–è®¾å¤‡è®¡ç®—èƒ½åŠ›
        major, minor = device.compute_capability()
        print(f"   ğŸ“Š è®¡ç®—èƒ½åŠ›: {major}.{minor}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def test_onnx_to_tensorrt_conversion(models_dir="tlr_control/models"):
    """æµ‹è¯•ONNXåˆ°TensorRTè½¬æ¢"""
    print("\nğŸ”„ æµ‹è¯•ONNXåˆ°TensorRTè½¬æ¢...")
    
    model_path = os.path.join(models_dir, "model_walk.onnx")
    if not os.path.exists(model_path):
        print(f"   âŒ ONNXæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False, None
    
    print(f"   ğŸ“ ONNXæ¨¡å‹: {model_path}")
    
    try:
        # ä½¿ç”¨FP16ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼ˆç‰¹åˆ«æ˜¯åœ¨Jetsonä¸Šï¼‰
        policy = PolicyModel(model_path, device="cuda", fp16=True)
        print(f"   âœ… TensorRTæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æ˜¾ç¤ºå¼•æ“ä¿¡æ¯
        engine_info = policy.get_engine_info()
        print(f"   ğŸ“Š å¼•æ“è·¯å¾„: {engine_info['engine_path']}")
        print(f"   ğŸ“Š FP16å¯ç”¨: {engine_info['fp16_enabled']}")
        print(f"   ğŸ“Š æœ€å¤§æ‰¹æ¬¡: {engine_info['max_batch_size']}")
        
        return True, policy
        
    except Exception as e:
        print(f"   âŒ TensorRTè½¬æ¢å¤±è´¥: {e}")
        return False, None

def test_inference_performance(policy, num_tests=50):
    """æµ‹è¯•æ¨ç†æ€§èƒ½"""
    print(f"\nâš¡ æµ‹è¯•æ¨ç†æ€§èƒ½ ({num_tests}æ¬¡)...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_obs = {
        "quaternion": [1.0, 0.0, 0.0, 0.0],
        "linear_vel": [0.3, 0.0, 0.0],
        "angular_vel": [0.0, 0.0, 0.0],
        "joint_pos": [0.1, -0.2, 0.1, -0.2, 0.1, -0.2],
        "joint_vel": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        "height": 0.25,
        "prev_height": 0.25,
        "prev_joint_vel": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        "joint_torque": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        "prev_action": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        "commands": [0.3, 0.0, 0.0]
    }
    
    import time
    inference_times = []
    
    # é¢„çƒ­
    for _ in range(5):
        policy.predict(test_obs)
    
    # æ­£å¼æµ‹è¯•
    for i in range(num_tests):
        # æ·»åŠ å°çš„éšæœºå˜åŒ–
        test_obs["joint_pos"] = [p + np.random.normal(0, 0.01) for p in test_obs["joint_pos"]]
        
        start_time = time.time()
        action = policy.predict(test_obs)
        end_time = time.time()
        
        inference_times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        if i == 0:
            print(f"   ğŸ¯ é¦–æ¬¡æ¨ç†ç»“æœ: {action}")
            print(f"   ğŸ“Š åŠ¨ä½œç»´åº¦: {action.shape}")
    
    # ç»Ÿè®¡ç»“æœ
    inference_times = np.array(inference_times)
    avg_time = np.mean(inference_times)
    std_time = np.std(inference_times)
    min_time = np.min(inference_times)
    max_time = np.max(inference_times)
    
    print(f"   ğŸ“ˆ å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"   ğŸƒ æœ€å¿«æ¨ç†æ—¶é—´: {min_time:.2f} ms")
    print(f"   ğŸŒ æœ€æ…¢æ¨ç†æ—¶é—´: {max_time:.2f} ms")
    print(f"   ğŸ”¥ æ¨ç†é¢‘ç‡: {1000/avg_time:.1f} Hz")
    
    # æ€§èƒ½è¯„ä¼°
    if avg_time < 2.0:
        print(f"   ğŸ‰ æ€§èƒ½ä¼˜ç§€! (< 2ms)")
    elif avg_time < 5.0:
        print(f"   âœ… æ€§èƒ½è‰¯å¥½! (< 5ms)")
    elif avg_time < 10.0:
        print(f"   âš ï¸  æ€§èƒ½ä¸­ç­‰ (< 10ms)")
    else:
        print(f"   ğŸ”´ æ€§èƒ½è¾ƒæ…¢ (> 10ms)")
    
    return avg_time < 10.0  # è®¤ä¸º10msä»¥ä¸‹ä¸ºå¯æ¥å—

def quick_test(models_dir="tlr_control/models"):
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ TensorRTç­–ç•¥æ¨¡å‹å¿«é€Ÿæµ‹è¯•")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        return False
    
    # 2. æµ‹è¯•è½¬æ¢
    success, policy = test_onnx_to_tensorrt_conversion(models_dir)
    if not success:
        return False
    
    # 3. æµ‹è¯•æ¨ç†æ€§èƒ½
    if not test_inference_performance(policy):
        print("\nâš ï¸  æ¨ç†æ€§èƒ½ä¸ç¬¦åˆè¦æ±‚ï¼Œä½†åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ TensorRTå¿«é€Ÿæµ‹è¯•å®Œæˆ!")
    return True

def print_jetson_optimization_tips():
    """æ‰“å°Jetsonä¼˜åŒ–å»ºè®®"""
    print("\nğŸ’¡ Jetsonæ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    print("   1. è®¾ç½®æœ€å¤§æ€§èƒ½æ¨¡å¼:")
    print("      sudo nvpmodel -m 0")
    print("      sudo jetson_clocks")
    print("   2. ç¡®ä¿ä½¿ç”¨FP16ç²¾åº¦ (å·²å¯ç”¨)")
    print("   3. è€ƒè™‘ä½¿ç”¨DLA (æ·±åº¦å­¦ä¹ åŠ é€Ÿå™¨)")
    print("   4. è°ƒæ•´TensorRTå·¥ä½œç©ºé—´å¤§å°")
    print("   5. ä½¿ç”¨INT8é‡åŒ– (å¦‚æœæ¨¡å‹æ”¯æŒ)")

if __name__ == "__main__":
    print("å½“å‰å·¥ä½œç›®å½•:", os.getcwd())
    
    # æ™ºèƒ½æ£€æŸ¥æ¨¡å‹ç›®å½•
    models_path = None
    if os.path.exists("tlr_control/models"):
        # ä»çˆ¶ç›®å½•è¿è¡Œ
        models_path = "tlr_control/models"
    elif os.path.exists("models"):
        # ä»tlr_controlç›®å½•å†…è¿è¡Œ
        models_path = "models"
    elif os.path.exists("../models"):
        # ä»å­ç›®å½•è¿è¡Œ
        models_path = "../models"
    
    if models_path is None:
        print("é”™è¯¯: æ‰¾ä¸åˆ°modelsç›®å½•")
        print("è¯·ç¡®ä¿ä»¥ä¸‹ç›®å½•ä¹‹ä¸€å­˜åœ¨:")
        print("  - tlr_control/models (ä»çˆ¶ç›®å½•è¿è¡Œ)")
        print("  - models (ä»tlr_controlç›®å½•è¿è¡Œ)")
        print("  - ../models (ä»å­ç›®å½•è¿è¡Œ)")
        sys.exit(1)
    
    print(f"æ‰¾åˆ°æ¨¡å‹ç›®å½•: {models_path}")
    
    success = quick_test(models_path)
    
    if not success:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print_jetson_optimization_tips()
        sys.exit(1)
    else:
        print("ğŸ’¯ TensorRTåŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œæ€§èƒ½ä¼˜å¼‚!")
        print("ğŸ”¥ å¯ä»¥ç»§ç»­éƒ¨ç½²åˆ°å®é™…æœºå™¨äººæ§åˆ¶ä¸­")
        print_jetson_optimization_tips() 