#!/usr/bin/env python3
"""
å°†Isaac Labè®­ç»ƒçš„.ptæ¨¡å‹æ–‡ä»¶è½¬æ¢ä¸ºONNXæ ¼å¼

ä½¿ç”¨æ–¹æ³•:
python convert_pt_to_onnx.py <input_pt_file> [output_dir] [--verbose]

ç¤ºä¾‹:
python convert_pt_to_onnx.py model.pt ./models/ --verbose
"""

import os
import sys
import torch
import argparse
from pathlib import Path

# å¯¼å…¥Isaac Labçš„å¯¼å‡ºå™¨
try:
    from source.isaaclab_rl.isaaclab_rl.rsl_rl.exporter import export_policy_as_onnx
    print("âœ… æˆåŠŸå¯¼å…¥Isaac Labå¯¼å‡ºå™¨")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥Isaac Labå¯¼å‡ºå™¨: {e}")
    print("è¯·ç¡®ä¿æ‚¨åœ¨Isaac Labç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

def load_model_checkpoint(pt_file_path):
    """åŠ è½½.ptæ¨¡å‹æ–‡ä»¶"""
    try:
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹æ–‡ä»¶: {pt_file_path}")
        checkpoint = torch.load(pt_file_path, map_location='cpu')
        print(f"âœ… æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # æ‰“å°checkpointçš„ç»“æ„ä¿¡æ¯
        print(f"ğŸ“‹ CheckpointåŒ…å«çš„é”®: {list(checkpoint.keys())}")
        
        return checkpoint
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æ–‡ä»¶å¤±è´¥: {e}")
        return None

def extract_policy_components(checkpoint):
    """ä»checkpointä¸­æå–ç­–ç•¥ç»„ä»¶"""
    try:
        # å¸¸è§çš„é”®åæ¨¡å¼
        possible_keys = [
            'model',           # é€šå¸¸çš„æ¨¡å‹é”®
            'actor_critic',    # Isaac Labå¸¸ç”¨
            'policy',          # ç­–ç•¥é”®
            'state_dict',      # çŠ¶æ€å­—å…¸
        ]
        
        model_state = None
        for key in possible_keys:
            if key in checkpoint:
                model_state = checkpoint[key]
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹çŠ¶æ€: {key}")
                break
        
        if model_state is None:
            print("âŒ æœªæ‰¾åˆ°æ¨¡å‹çŠ¶æ€")
            return None, None
        
        # æŸ¥æ‰¾normalizer
        normalizer = None
        if 'obs_normalizer' in checkpoint:
            normalizer = checkpoint['obs_normalizer']
            print("âœ… æ‰¾åˆ°è§‚æµ‹å½’ä¸€åŒ–å™¨")
        elif 'normalizer' in checkpoint:
            normalizer = checkpoint['normalizer']
            print("âœ… æ‰¾åˆ°å½’ä¸€åŒ–å™¨")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å½’ä¸€åŒ–å™¨ï¼Œå°†ä½¿ç”¨Identity")
        
        return model_state, normalizer
        
    except Exception as e:
        print(f"âŒ æå–ç­–ç•¥ç»„ä»¶å¤±è´¥: {e}")
        return None, None

def create_dummy_actor_critic(state_dict):
    """æ ¹æ®state_dictåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„actor_criticå¯¹è±¡"""
    
    class DummyActorCritic:
        def __init__(self, state_dict):
            self.actor = torch.nn.Sequential()
            self.is_recurrent = False
            
            # å°è¯•é‡å»ºç½‘ç»œç»“æ„
            self._build_network_from_state_dict(state_dict)
            
        def _build_network_from_state_dict(self, state_dict):
            """ä»state_dicté‡å»ºç½‘ç»œ"""
            # è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨çš„å…·ä½“æ¨¡å‹ç»“æ„æ¥è°ƒæ•´
            # å‡è®¾æ˜¯æ ‡å‡†çš„å…¨è¿æ¥ç½‘ç»œ
            
            layers = []
            weight_keys = [k for k in state_dict.keys() if 'weight' in k and 'actor' in k]
            weight_keys.sort()
            
            print(f"ğŸ“‹ æ‰¾åˆ°æƒé‡å±‚: {weight_keys}")
            
            for i, weight_key in enumerate(weight_keys):
                weight = state_dict[weight_key]
                bias_key = weight_key.replace('weight', 'bias')
                
                if len(weight.shape) == 2:  # å…¨è¿æ¥å±‚
                    in_features, out_features = weight.shape[1], weight.shape[0]
                    layer = torch.nn.Linear(in_features, out_features)
                    
                    # åŠ è½½æƒé‡å’Œåç½®
                    layer.weight.data = weight
                    if bias_key in state_dict:
                        layer.bias.data = state_dict[bias_key]
                    
                    layers.append(layer)
                    
                    # æ·»åŠ æ¿€æ´»å‡½æ•°ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
                    if i < len(weight_keys) - 1:
                        layers.append(torch.nn.ReLU())
            
            if layers:
                self.actor = torch.nn.Sequential(*layers)
                print(f"âœ… é‡å»ºç½‘ç»œç»“æ„: {len(layers)}å±‚")
            else:
                print("âŒ æ— æ³•é‡å»ºç½‘ç»œç»“æ„")
    
    return DummyActorCritic(state_dict)

def convert_pt_to_onnx(pt_file_path, output_dir="./", filename=None, verbose=False):
    """å°†.ptæ–‡ä»¶è½¬æ¢ä¸ºONNXæ ¼å¼"""
    
    # 1. åŠ è½½checkpoint
    checkpoint = load_model_checkpoint(pt_file_path)
    if checkpoint is None:
        return False
    
    # 2. æå–ç»„ä»¶
    model_state, normalizer = extract_policy_components(checkpoint)
    if model_state is None:
        return False
    
    # 3. åˆ›å»ºactor_criticå¯¹è±¡
    print("ğŸ”§ é‡å»ºæ¨¡å‹ç»“æ„...")
    actor_critic = create_dummy_actor_critic(model_state)
    
    # 4. ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    if filename is None:
        pt_filename = Path(pt_file_path).stem
        filename = f"{pt_filename}.onnx"
    
    # 5. æ‰§è¡Œè½¬æ¢
    try:
        print(f"ğŸ”„ å¼€å§‹è½¬æ¢ä¸ºONNXæ ¼å¼...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {filename}")
        
        export_policy_as_onnx(
            actor_critic=actor_critic,
            path=output_dir,
            normalizer=normalizer,
            filename=filename,
            verbose=verbose
        )
        
        output_path = os.path.join(output_dir, filename)
        print(f"âœ… è½¬æ¢æˆåŠŸ! ONNXæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="å°†Isaac Lab .ptæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼")
    parser.add_argument("input_file", help="è¾“å…¥çš„.ptæ–‡ä»¶è·¯å¾„")
    parser.add_argument("output_dir", nargs='?', default="./", help="è¾“å‡ºç›®å½• (é»˜è®¤: ./)")
    parser.add_argument("--filename", help="è¾“å‡ºONNXæ–‡ä»¶å (é»˜è®¤: åŸºäºè¾“å…¥æ–‡ä»¶å)")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # æ‰§è¡Œè½¬æ¢
    print("ğŸš€ å¼€å§‹æ¨¡å‹è½¬æ¢...")
    success = convert_pt_to_onnx(
        pt_file_path=args.input_file,
        output_dir=args.output_dir,
        filename=args.filename,
        verbose=args.verbose
    )
    
    if success:
        print("ğŸ‰ è½¬æ¢å®Œæˆ!")
    else:
        print("ğŸ’¥ è½¬æ¢å¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main() 